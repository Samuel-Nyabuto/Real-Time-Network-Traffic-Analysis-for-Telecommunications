{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-6dGMGy8xMS",
        "outputId": "e322dac6-a851-4b38-e62f-5c72a105347c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/246.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m245.8/246.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=42f26cbb9fac5ed83060318f106496df4dd254f138e3aba594d9d8dd77e28e55\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting confluent-kafka\n",
            "  Downloading confluent_kafka-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install kafka-python\n",
        "!pip install pyspark\n",
        "!pip install confluent-kafka\n",
        "!pip install streamlit\n",
        "!pip install kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Wg4xw2zkhPZX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from confluent_kafka import Consumer, KafkaError, Producer\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Real-Time Network Traffic Analysis for Telecommunications\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configure Kafka connection details\n",
        "bootstrap_servers = 'pkc-6ojv2.us-west4.gcp.confluent.cloud:90922'\n",
        "sasl_username = 'DTD7CDW7KYC6A5LW'\n",
        "sasl_password = 'apbM5BtlA57GiGcbBmgTZbT+tRxgVqyamXb6uhpBZm/z1RFm/E7F8buyelzboYs5'\n",
        "kafka_topic = 'network-traffic'\n",
        "processed_topic = 'processed-data'\n",
        "\n",
        "# Define schema for the data\n",
        "schema = StructType([\n",
        "    StructField(\"source_ip\", StringType(), True),\n",
        "    StructField(\"destination_ip\", StringType(), True),\n",
        "    StructField(\"bytes_sent\", IntegerType(), True),\n",
        "    StructField(\"event_time\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Create Kafka consumer configuration\n",
        "conf = {\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.mechanisms': 'PLAIN',\n",
        "    'sasl.username': sasl_username,\n",
        "    'sasl.password': sasl_password,\n",
        "    'group.id': 'network-traffic-group',\n",
        "    'auto.offset.reset': 'earliest'\n",
        "}\n",
        "\n",
        "# Create Kafka consumer\n",
        "consumer = Consumer(conf)\n",
        "producer = Producer(conf)\n",
        "\n",
        "# Subscribe to the Kafka topic\n",
        "consumer.subscribe([kafka_topic])\n",
        "\n",
        "# Read data from Kafka and perform real-time visualization\n",
        "def read_from_kafka():\n",
        "    # Create a Streamlit app\n",
        "    st.title(\"Real-Time Network Traffic Analysis for Telecommunications\")\n",
        "\n",
        "    # Create a plot to visualize processed data\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Initialize an empty list to store processed data and event times\n",
        "    processed_data = []\n",
        "    event_times = []\n",
        "\n",
        "    # Function to process incoming Kafka messages and update the plot\n",
        "    def process_message(message):\n",
        "        nonlocal processed_data, event_times\n",
        "        if message is None:\n",
        "            return\n",
        "        if message.error():\n",
        "            if message.error().code() == KafkaError._PARTITION_EOF:\n",
        "                return\n",
        "            else:\n",
        "                print(f\"Error: {message.error()}\")\n",
        "                return\n",
        "\n",
        "        value = message.value().decode('utf-8')\n",
        "        data = value.split(',')\n",
        "        source_ip = data[0]\n",
        "        destination_ip = data[1]\n",
        "        bytes_sent = int(data[2])\n",
        "        event_time = datetime.now()\n",
        "\n",
        "        row = (source_ip, destination_ip, bytes_sent, event_time)\n",
        "        df = spark.createDataFrame([row], schema=schema)\n",
        "\n",
        "        # Perform window-based aggregations\n",
        "        aggregated_df = df \\\n",
        "            .groupBy(window(\"event_time\", \"1 minute\"), \"source_ip\") \\\n",
        "            .agg(sum(\"bytes_sent\").alias(\"total_bytes_sent\")) \\\n",
        "            .orderBy(desc(\"total_bytes_sent\"))\n",
        "\n",
        "        # Convert the aggregated DataFrame to Pandas DataFrame for plotting\n",
        "        pd_df = aggregated_df.toPandas()\n",
        "\n",
        "        # Process the Kafka message and update the plot\n",
        "        processed_data.append(bytes_sent)\n",
        "        event_times.append(event_time)\n",
        "        ax.clear()\n",
        "        ax.plot(event_times, processed_data)\n",
        "        ax.set_xlabel(\"Event Time\")\n",
        "        ax.set_ylabel(\"Information\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    # Continuously read messages from Kafka topic\n",
        "    while True:\n",
        "        message = consumer.poll(1.0)\n",
        "        if message is None:\n",
        "            continue\n",
        "        if message.error():\n",
        "            if message.error().code() == KafkaError._PARTITION_EOF:\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Error: {message.error()}\")\n",
        "                break\n",
        "\n",
        "        process_message(message)\n",
        "\n",
        "\n",
        "# Publish network traffic data to Kafka topic\n",
        "def publish_to_kafka(source_ip, destination_ip, bytes_sent):\n",
        "    producer.produce(kafka_topic, f\"{source_ip},{destination_ip},{bytes_sent}\".encode('utf-8'))\n",
        "    producer.flush()\n",
        "\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == '__main__':\n",
        "    read_from_kafka()\n"
      ],
      "metadata": {
        "id": "Ba1g-_S4-EFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pW2vOzbBztSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}